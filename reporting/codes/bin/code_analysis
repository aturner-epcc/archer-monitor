#!/usr/bin/env python
#
#===============================================================
# code_analysis
#
# Analyse code usage from captured apstat output
#===============================================================
#
#===============================================================
# v0.1 - Refactoring code_usage script for more flexible analysis
#===============================================================
#
#----------------------------------------------------------------------
# Copyright 2015 EPCC, The University of Edinburgh
#
# This file is part of archer-monitoring.
#
# archer-monitoring is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# archer-monitoring is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with archer-monitoring.  If not, see <http://www.gnu.org/licenses/>.
#----------------------------------------------------------------------
#
# For help see function at bottom of source
#
__author__ = 'Andrew Turner, EPCC, The University of Edinburgh'
__version__ = '0.3'

# Import the required modules
from code_def import CodeDef
from timelineplot import get_filelist
from datetime import datetime, timedelta
import pandas as pd
import numpy as np

import matplotlib
matplotlib.use('Agg')
from matplotlib import pyplot as plt
matplotlib.rcParams.update({'font.size': 8})
matplotlib.rcParams.update({'figure.autolayout': True})
import matplotlib.colors as colors
import matplotlib.cm as cmx
import seaborn as sns


import err_handle as error
import sys
import os
import fnmatch
import getopt
import subprocess
import ConfigParser
import grp
import re
import time
import prettytable

kau = 0.36

def main(argv):

    #=======================================================
    # Global configuration section
    #=======================================================
    # This needs to be set so we know where to get the code descriptions
    rootDir = os.environ['ARCHER_MON_BASEDIR']
    # Where to place the output files (use env variable or current directory)
    outputDir = os.environ.get('ARCHER_MON_OUTDIR', '.')
    outDir = os.environ['PWD']
    size_labels = ["Small", "Medium", "Large", "V. Large", "Huge"]
    # Default base unit
    size_unit = "Nodes"
    size_scale = 1.0
    use_unit = "Node Hours"
    use_scale = 1.0
    # Default prefix for output file names
    prefix = "app"
    # Default log location
    DEFAULT_LOGFILE_DIR = "/home/z01/z01/archstat/archer-monitor/logs/applications"

    #=======================================================
    # Read any code definitions
    #=======================================================
    codeConfigDir = rootDir + '/reporting/codes/descriptions'
    codes = []
    nCode = 0
    # Create a dictionary of codes
    codeDict = {}
    for file in os.listdir(codeConfigDir):
        if fnmatch.fnmatch(file, '*.code'):
            nCode += 1
            code = CodeDef()   
            code.readConfig(codeConfigDir + '/' + file)
            codes.append(code)
            codeDict[code.name] = nCode - 1

    # Here we loop over all defined codes zeroing the appropriate variables and dicts
    users = ''

    #=======================================================
    # Command line options
    #=======================================================
    # Read the command-line options
    try:
        opts, args = getopt.getopt(argv, "l:s:e:c:g:x:k:p:vuh", 
                 ["logdir=", "start=", "end=", "cores=", "group=", "notgroup=", "kau=", "prefix=", 
                  "csv", "users", "help"])
    except getopt.GetoptError:
        error.handleError("Could not parse command line options\n")

    # Parse the command-line options
    bygroup = False
    bynotgroup = False
    grouplist = None
    csv = False
    listusers = False
    startfile = datetime.utcfromtimestamp(0)
    endfile = datetime.now()
    indir = DEFAULT_LOGFILE_DIR
    for opt, arg in opts:
        if opt in ("-c", "--cores"):
            size_unit = "Cores"
            size_scale = float(arg.strip())
        if opt in ("-k", "--kau"):
            use_unit = "kAU"
            use_scale = float(arg.strip())
        elif opt in ("-l", "--logdir"):
            indir = arg.strip()
        elif opt in ("-s", "--start"):
            startfile = datetime.strptime(arg, "%Y-%m-%d")
        elif opt in ("-e", "--end"):
            endfile = datetime.strptime(arg, "%Y-%m-%d")
        elif opt in ("-g", "--group"):
            print "   Getting usage for groups = {0}".format(arg)
            grouplist = arg.split(",")
            bygroup = True
        elif opt in ("-x", "--notgroup"):
            print "   Excluding usage for groups = {0}".format(arg)
            grouplist = arg.split(",")
            bynotgroup = True
        elif opt in ("-p", "--prefix"):
            prefix = arg.strip()
        elif opt in ("-u", "--users"):
            listusers = True
        elif opt in ("-v", "--csv"):
            csv = True
        elif opt in ("-h", "--help"):
            printHelp(rootDir)
            exit(0)

    sys.stderr.write("""
===========================================================================
code_analysis {0}
---------------------------------------------------------------------------
Copyright 2012-2016 EPCC, The University of Edinburgh
This program comes with ABSOLUTELY NO WARRANTY. This is free software, and
you are welcome to redistribute it under certain conditions. See the GNU
General Public License for more details: <http://www.gnu.org/licenses/>.
===========================================================================
""".format(__version__))
    sys.stdout.write("Analysing data in: {0}\n".format(indir))
    if size_scale > 1:
       print "Cores per node: ", size_scale
    if grouplist is not None:
       print "Restricting usage to group(s): " + str(grouplist)

    # Check the directory exists
    if not os.path.isdir(indir):
       print "**ERROR - Log directory is not valid: " + indir
       printHelp(rootDir)
       exit(0)

    # Read the logfile directory (assume files have extension 'apstat')
    files = get_filelist(indir, 'apstat')

    startdate = None
    enddate = None

    #=======================================================
    # Initialise the arrays for storing the total data
    #=======================================================
    totjobs = 0
    totnh = 0
    matchedjobs = {}
    usergroup = {}
    jobframe_proto = []
    tot_nodelist = []
    code_nodelist = {}
    for code in codes:
       name = code.name
       code_nodelist[name] = []
    code_nodelist['Unidentified'] = []

    #=======================================================
    # Loop over all log files found in the specified path
    #=======================================================
    for file in files:
       filename = os.path.basename(file)
       filedate = filename.split('.')[0]
       fdate = datetime.strptime(filedate, "%Y-%m-%d")

       #=======================================================
       # Check if this file is in the specified date range
       #=======================================================
       if fdate >= startfile and fdate <= endfile:
          if startdate is None: startdate = fdate
          enddate = fdate

          #=======================================================
          # Open this logfile 
          #=======================================================
          logFile = open(file, "r")
          print file

          # Loop over lines in the file reading them
          for line in logFile:

             if re.match("__START", line) is not None:
                timeline = line.rstrip()
                tokens = timeline.split()
             # End of if start block match
               
             # Skip the end indicator
             if re.match("__END", line) is not None:
                 continue

             # Extract the useful information from the log
             line = line.rstrip()
             tokens = line.split()

             if len(tokens) < 8:
                continue
     
             keepline = True

             jobid = tokens[0]
             user = tokens[2]
             nodes = int(tokens[4]) 
             exename = tokens[7]

             # Assign a project based on the username
             project = None
             if user in usergroup:
                project = usergroup[user]
             else:
                bashCommand = "id -Gn {0}".format(user)
                getgroup = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)
                groups = getgroup.communicate()[0]
                if groups:
                   project = groups.split()[0]
                   usergroup[user] = project
                else:
                   project = 'Unknown'
                   usergroup[user] = project
          
             if bygroup:
                if project not in grouplist:
                   keepline = False
             if bynotgroup:
                if project in grouplist:
                   keepline = False

             # Try to match the code
             codename = "Unidentified"
             codelang = "Unidentified"
             codetype = "Unidentified"
             codearea = "Unidentified"
             codelic = "Unidentified"
             for code in codes:
                name = code.name
                # Test if this one of the known codes
                if re.search(code.regexp, exename):
                   codename = name
                   codelang = code.pri_lang
                   codetype = code.type
                   codearea = code.area
                   codelic = code.aca_licence

             # Temporary dict for storing this line
             jobdict = {'JobID': jobid,
                        'User': user,
                        'Project': project,
                        'Nodes': nodes,
                        'Exe': exename,
                        'Code': codename,
                        'Language': codelang,
                        'Type': codetype,
                        'Area': codearea,
                        'Licence': codelic,
                        'Jobs': 1,
                        'Nh': nodes}

             if keepline:
                totnh += nodes

                # This is so we can compute statistics weighted by Nh per code
                nodelist = nodes * [nodes]
                tot_nodelist.extend(nodelist)
                code_nodelist[codename].extend(nodelist)

                # Test if we already have this job
                jobidx = 0
                if jobid in matchedjobs:
                    # Yes, get the index and increment the node hours
                    jobidx = matchedjobs[jobid]
                    jobframe_proto[jobidx]['Nh'] += nodes
                else:
                    # No, set a new index and append to the proto frame
                    matchedjobs[jobid] = totjobs
                    jobidx = totjobs
                    totjobs += 1
                    jobframe_proto.append(jobdict)

  
          # End of loop over logfile lines
          
          # Close the logfile
          logFile.close()

    # End of loop over logfiles

    # Compute the major statistics
    totuse = totnh * use_scale

    # The main data frame describing the jobs
    jobframe = pd.DataFrame(jobframe_proto)

    # Get total node hours by job size
    size_bins = [4, 64, 256, 512, 4920]
    size_labels = []
    for i, size in enumerate(size_bins):
       label = None
       if i == 0:
          end = size_bins[i] * size_scale
          label = '({:.0f},{:.0f}]'.format(0, end)
       else:
          start = size_bins[i-1] * size_scale
          end = size_bins[i] * size_scale
          label = '({:.0f},{:.0f}]'.format(start, end)
       size_labels.append(label)
    # Need this to stop the labels being shifted on the plot
    size_labels.insert(0, '')
    size_hist = len(size_bins) * [0]
    job_hist = len(size_bins) * [0]
    sizesum = jobframe.groupby('Nodes').agg(np.sum)
    for size in sizesum.index:
       for i, e in enumerate(size_bins):
          if size <= e:
             if e in size_hist:
                size_hist[i] += sizesum.loc[size, 'Nh'] * use_scale
                job_hist[i] += sizesum.loc[size, 'Jobs']
             else:
                size_hist[i] = sizesum.loc[size, 'Nh'] * use_scale
                job_hist[i] = sizesum.loc[size, 'Jobs']
             break

    # Node hours
    fig, ax = plt.subplots()
    plt.bar(range(len(size_hist)), size_hist, facecolor='red', align='center')
    ax.set_xticklabels(size_labels)
    plt.xlabel('Job Size / ' + size_unit)
    plt.ylabel('Usage / ' + use_unit)
    plt.savefig(prefix + "_usebysize.png", dpi=300)
    plt.clf()
    # Jobs
    fig, ax = plt.subplots()
    plt.bar(range(len(job_hist)), job_hist, facecolor='red', align='center')
    ax.set_xticklabels(size_labels)
    plt.xlabel('Job Size / ' + size_unit)
    plt.ylabel('Jobs')
    plt.savefig(prefix + "_jobsbysize.png", dpi=300)
    plt.clf()

    # Get the summarised usage in cores
    codesum = jobframe.groupby('Code').agg(np.sum).sort_values('Nh')
    codesum = codesum.multiply(use_scale)
    usersum = jobframe.groupby('User').agg(np.sum).sort_values('Nh')

    # Extract the unidentified data and the top usage codes
    knowncodes = codesum.drop(codesum[codesum.index == 'Unidentified'].index)
    unknowncodes = codesum.drop(codesum[codesum.index != 'Unidentified'].index)
    ttopcode = knowncodes.loc[knowncodes['Nh'] >= 0.01*totuse]
    lowcode = knowncodes.loc[knowncodes['Nh'] < 0.01*totuse]

    # Add unidentified and other code details and re-sort
    ttopcode = ttopcode.sort_values('Nh')
    topcode = unknowncodes
    topcode.loc['Other'] = lowcode.sum()
    topcode = topcode.append(ttopcode)

    # Pie chart of codes
    plotPie(topcode['Nh'].tolist(), topcode.index, 0.02, 'code_pie.png')

    # Bar chart of codes
    topcode.plot(kind='barh', y='Nh', legend=False, facecolor='red')
    plt.ylabel('')
    plt.xlabel('Usage / ' + use_unit)
    plt.savefig(prefix + "_bar_usage.png", dpi=300)
    plt.clf()

    # Get the total number of unique users
    users = jobframe.User.ravel()
    totusers = len(pd.unique(users))
 
    # Get statistics for each code, unidentified and overall
    codeplot = []
    codelabels = []
    csvfile = None
    statstable = None
    csvfile = open(prefix + "_stats.csv", "w")
    st = statsHeaderCSV(csvfile)
    statstable = prettytable.PrettyTable(st)
    for name in reversed(knowncodes.index):
       if name == "Unidentified" or name == "Other":
          continue
       if len(code_nodelist[name]) > 0:
          users = jobframe.loc[jobframe['Code'] == name].User.ravel()
          nusers = len(pd.unique(users))
          st = statsReportCSV(name, codesum.loc[name]['Nh'], codesum.loc[name]['Jobs'] / use_scale, 
                         totuse, totjobs, nusers, [x*size_scale for x in code_nodelist[name]], csvfile)
          statstable.add_row(st)
          # Lists for boxplots
          if st[1] >= 1.0:
             codeplot.insert(0, [x*size_scale for x in code_nodelist[name]])
             codelabels.insert(0, name)
    if len(code_nodelist['Unidentified']) > 0:
       users = jobframe.loc[jobframe['Code'] == 'Unidentified'].User.ravel()
       nusers = len(pd.unique(users))
       st = statsReportCSV("Unidentified", codesum.loc['Unidentified']['Nh'],
                      codesum.loc['Unidentified']['Jobs'] / use_scale, 
                      totuse, totjobs, nusers, [x*size_scale for x in code_nodelist['Unidentified']],
                      csvfile)
       statstable.add_row(st)
       # Add to boxplot list
       codeplot.insert(0, [x*size_scale for x in code_nodelist['Unidentified']])
       codelabels.insert(0, 'Unidentified')

    # Series representing the total job size statistics
    tot_series = pd.Series(tot_nodelist)
    tot_series = tot_series.multiply(size_scale)
    st = statsReportCSV("Overall", totuse, totjobs, totuse, totjobs, totusers, 
                   [x*size_scale for x in tot_nodelist], csvfile)
    statstable.add_row(st)
    statstable.float_format = ".0"
    statstable.float_format["% Use"] = ".2"
    statstable.float_format["% Jobs"] = ".2"
    statstable.float_format["Mean"] = ".2"
    statstable.align = "r"
    statstable.align["Application"] = "c"
    print statstable
    csvfile.close()
    # Add to boxplot list
    codeplot.insert(0, tot_series)
    codelabels.insert(0, 'Overall')

    # Boxplots of codes
    plt.boxplot(codeplot, notch=0, sym="", showmeans=True, vert=False, whis=1.5, labels=codelabels)
    plt.xlabel("Job Size / " + size_unit)
    plt.savefig(prefix + "_sizebycode.png", dpi=300)
    plt.clf()
    # Boxplots of codes
    plt.boxplot(codeplot, notch=0, sym="", showmeans=True, vert=False, whis=1.5, labels=codelabels)
    plt.xlabel("Job Size / " + size_unit)
    plt.xlim([0,5000])
    plt.savefig(prefix + "_sizebycode_small.png", dpi=300)
    plt.clf()

    # List users associated with each application
    if listusers:
       userfile = open(prefix + '_userlist.md', 'w')
       # Known applications
       userfile.write('\n# User List: Known Applications #\n\n')
       for name in reversed(codesum.index):
          if name == 'Unidentified':
             continue
          users = pd.unique(jobframe.loc[jobframe['Code'] == name].User.ravel())
          userfile.write('\n## {} ##\n\n'.format(name))
          puse = 100.0 * codesum.loc[name]['Nh'] / totuse
          userfile.write('Usage {:.0f} {} ({:.2f}%)\n\n'.format(codesum.loc[name]['Nh'], use_unit, puse))
          for user in users:
             bashCommand = "finger {0}".format(user)
             getperson = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)
             person = getperson.communicate()[0].split(':')[2].split('\n')[0]
             userfile.write('* {}: {}\n'.format(user, person))

       # Unknown applications
       unknowncodes =  jobframe.loc[jobframe['Code'] == 'Unidentified']
       codesum = unknowncodes.groupby('Exe').agg(np.sum).sort_values('Nh')
       userfile.write('\n# User List: Unknown Applications #\n\n')
       for name in reversed(codesum.index):
          if codesum.loc[name]['Nh'] >= 0.01*totuse:
             userfile.write('\n## {} ##\n\n'.format(name))
             puse = 100.0 * codesum.loc[name]['Nh'] / totuse
             userfile.write('Usage {:d} {} ({:.2f}%)\n'.format(codesum.loc[name]['Nh'], use_unit, puse))
             users = pd.unique(jobframe.loc[jobframe['Exe'] == name].User.ravel())
             for user in users:
                bashCommand = "finger {0}".format(user)
                getperson = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)
                person = getperson.communicate()[0].split(':')[2].split('\n')[0]
                userfile.write('* {}: {}\n'.format(user, person))
       userfile.close()

    print 

    sys.exit(0)

#=============================================================
# Small function to report on stats based on list of job sizes
#=============================================================
def statsReportCSV(label, use, jobs, totuse, totjobs, nusers, datalist, csvfile):
    dataseries = pd.Series(datalist)
    pusage = 100.0 * use / totuse
    pjobs = 100.0 * jobs / totjobs
    min = dataseries.min()
    q1 = dataseries.quantile(0.25)
    median = dataseries.quantile()
    q3 = dataseries.quantile(0.75)
    max = dataseries.max()
    mean = dataseries.mean()
    csvfile.write('"{}",{:.2f}%,{:.3f},{:.2f}%,{:.0f},{:.0f},{:.0f},{:.0f},{:.0f},{:.0f},{:.2f},{}\n'.format(label, 
                   pusage, use*kau, pjobs, jobs, min, q1, median, q3, max, mean, nusers))
    return (label, pusage, pjobs, min, q1, median, q3, max, mean, nusers)

#=============================================================
# Print header line for stats
#=============================================================
def statsHeaderCSV(csvfile):
    csvfile.write('"Application","% Use","Use (kAU)","% Jobs","Jobs","Min.","Q1","Median","Q3","Max.","Mean","Users"\n')
    return ("Application","% Use","% Jobs","Min.","Q1","Median","Q3","Max.","Mean","Users")

#=======================================================
# Function to save pie chart of specified values
#=======================================================
def plotPie(values, labels, limit, file):
    # Set up a colormap for pie charts
    pie_colmap = cm = plt.get_cmap('Pastel2') 
    pie_cNorm  = colors.Normalize(vmin=0, vmax=len(values))
    pie_scalarMap = cmx.ScalarMappable(norm=pie_cNorm, cmap=pie_colmap)
    fig = plt.figure(1)
    fig.clf()
    ax = plt.subplot(1, 1, 1)
    pie_vals = []
    pie_cols = []
    pie_explode = []
    pie_labels = []
    sum_vals = float(sum(values))
    for i, value in enumerate(values):
         pie_cols.append(pie_scalarMap.to_rgba(i))
         pie_vals.append(value)
         pie_explode.append(0.05) 
         # Add the % to wedge labels.
         pie_labels.append("{} ({:.1f}%)".format(labels[i], 100.0*value/sum_vals))
    ax.pie(pie_vals, explode=pie_explode, labels=pie_labels, colors=pie_cols,
           autopct='', wedgeprops={'linewidth': 0})
#           pctdistance=0.9, autopct=autopct_generator(limit*100.0))
    ax.axis('equal')
    fig.savefig(file, dpi=300, bbox_inches='tight')


#=======================================================
# Function generate function to automatically suppress
# % labels on pie chart below defined %age
#=======================================================
def autopct_generator(limit):
    def inner_autopct(pct):
        return ('%.1f%%' % pct) if pct > limit else ''
    return inner_autopct

#=======================================================
# Function to print help
#=======================================================
def printHelp(rootDir):
    """Print help for the tool.
           
           Arguments:
              str rootDir - The root install directory of the tool.
        """

    print """Get code usage statistics from specified log file.

USAGE

code_analysis [options]

OPTIONS

-l,--logdir <log directory> Directory containing the logfiles to analyse. The
                            default is:
                            /home/z01/z01/archstat/archer-monitor/logs/applications

-c,--cores  <number>        Report data in CPUh instead of node hours (Nh). The
                            number specifies the number of cores per node for the
                            conversion.

-s,--start  yyyy-mm-dd      Start date for analysis (default is start of data in logfiles) 
-e,--end    yyyy-mm-dd      End date for analysis (default is end of data in logfiles) 

-g,--group  <group1>,<group2> Restrict analysis to specified groups (e.g. n01,n02)

--help                      Show this help.
"""

if __name__ == "__main__":
    main(sys.argv[1:])
