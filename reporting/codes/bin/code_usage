#!/usr/bin/env python
#
#===============================================================
# code_usage
#
# Analyse code usage from logs
#===============================================================
#
#===============================================================
# v0.4 - Modified to work all z01 users
# v0.3 - Statistics on code usage over time
# v0.2 - Read from separate log files (one for each day)
# v0.1 - Initial version
#===============================================================
#
#----------------------------------------------------------------------
# Copyright 2014 EPCC, The University of Edinburgh
#
# This file is part of archer-monitoring.
#
# archer-monitoring is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# archer-monitoring is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with archer-monitoring.  If not, see <http://www.gnu.org/licenses/>.
#----------------------------------------------------------------------
#
# For hepl see function at bottom of source
#
__author__ = 'Andrew Turner, EPCC, The University of Edinburgh'
__version__ = '0.4'

# Import the required modules
from code_def import CodeDef
from timelineplot import get_filelist
from datetime import datetime, timedelta
import err_handle as error
import sys
import os
import fnmatch
import getopt
import subprocess
import ConfigParser
import grp
import re
import time

def main(argv):


    #=======================================================
    # Global configuration section
    #=======================================================
    rootDir = os.environ['ARCHER_MON_BASEDIR']
    outDir = os.environ['PWD']
    # Sizes should be in nodes
    sizes = [4, 64, 256, 512, 4920]
    size_labels = ["Small", "Medium", "Large", "V. Large", "Huge"]
    thresh = 0.005
    day_interval = 14
    ntop = 10
    units = "nodes"
    punit = "Nh"
    uscale = 1
    html = False
    # Default logfile directory
    DEFAULT_LOGFILE_DIR = "/home/z01/z01/archstat/archer-monitor/logs/applications"

    #=======================================================
    # Read any code definitions
    #=======================================================
    codeConfigDir = rootDir + '/reporting/codes/descriptions'
    codes = []
    nCode = 0
    # We also need to create a dictionary of resources here
    codeDict = {}
    for file in os.listdir(codeConfigDir):
        if fnmatch.fnmatch(file, '*.code'):
            nCode += 1
            code = CodeDef()   
            code.readConfig(codeConfigDir + '/' + file)
            codes.append(code)
            codeDict[code.name] = nCode - 1

    # Here we loop over all defined codes zeroing the appropriate variables and dicts
    totjobs = 0
    totnh = 0
    totusers = 0
    tot_nh_bysize = {}
    tot_jobs_bysize = {}
    matchedjobs = {}
    users = ''
    code_regexp = {}
    code_users = {}
    code_num_users = {}
    code_nh = {}
    code_nh_bysize = {}
    code_jobs = {}
    code_jobs_bysize = {}
    other_nh = 0
    other_users = ""
    other_num_users = 0
    other_nh_bysize = {}
    other_jobs = 0
    other_jobs_bysize = {}
    other_codes = {}
    code_nh_bysize = {}
    code_jobs_bysize = {}
    for size in sizes:
       tot_nh_bysize[size] = 0
       tot_jobs_bysize[size] = 0
       other_nh_bysize[size] = 0
       other_jobs_bysize[size] = 0
    for code in codes:
       name = code.name
       code_regexp[name] = code.regexp
       code_nh[name] = 0
       code_jobs[name] = 0
       code_users[name] = ''
       code_num_users[name] = 0

    #=======================================================
    # Command line options
    #=======================================================
    # Read the command-line options
    try:
        opts, args = getopt.getopt(argv, "b:l:i:s:e:c:g:qh", 
                 ["top=", "thresh=", "bins=", "logdir=", "interval=", "start=",
                  "end=", "cores=", "group=", "html", "quiet", "noplot", "csv", "help"])
    except getopt.GetoptError:
        error.handleError("Could not parse command line options\n")

    # Parse the command-line options
    byuser = False
    csv = False
    png = True
    text = True
    userlist = []
    group = None
    startfile = datetime.utcfromtimestamp(0)
    endfile = datetime.now()
    indir = DEFAULT_LOGFILE_DIR
    for opt, arg in opts:
        if opt in ("-c", "--cores"):
            units = "cores"
            punit = "CPUh"
            uscale = int(arg)
        elif opt in ("-q", "--quiet"):
            text = False
        elif opt in ("--top"):
            ntop = int(arg)
        elif opt in ("--noplot"):
            png = False
        elif opt in ("--html"):
            html = True
        elif opt in ("--csv"):
            csv = True
        elif opt in ("-b", "--bins"):
            # Try to read in custom size bins from file
            if os.path.isfile(arg):
               binfile = open(arg, "r")
               line = binfile.readline()
               sizes = line.rstrip().split()
               # Must convert from string to int for this to work
               sizes = [int(size) for size in sizes]
               line = binfile.readline()
               size_labels = line.rstrip().split()
               binfile.close()
            else:
               print "**ERROR - Histogram bins file is not valid: " + arg
               printHelp(rootDir)
               exit(0)
        elif opt in ("--thresh"):
            thresh = float(arg)
        elif opt in ("-l", "--logdir"):
            indir = arg
        elif opt in ("-i", "--interval"):
            day_interval = int(arg)
        elif opt in ("-s", "--start"):
            startfile = datetime.strptime(arg, "%Y-%m-%d")
        elif opt in ("-e", "--end"):
            endfile = datetime.strptime(arg, "%Y-%m-%d")
        elif opt in ("-g", "--group"):
            group = arg
            bashCommand = "ldapsearch -x cn={0}".format(arg)
            ldap = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)
            users = ldap.communicate()[0]
            for line in users.splitlines():
               if "memberUid" in line:
                  line = line.rstrip()
                  tokens = line.split()
                  userlist.append('"' + tokens[1] + '"')
            byuser = True
        elif opt in ("-h", "--help"):
            printHelp(rootDir)
            exit(0)


    if text:
       #=======================================================
       # Print out the banner
       #=======================================================
       sys.stderr.write("""
===========================================================================
code_usage {0}
---------------------------------------------------------------------------
Copyright 2012-2014  EPCC, The University of Edinburgh
This program comes with ABSOLUTELY NO WARRANTY. This is free software, and
you are welcome to redistribute it under certain conditions. See the GNU
General Public License for more details: <http://www.gnu.org/licenses/>.
===========================================================================
""".format(__version__))
       sys.stdout.write("Analysing data in: {0}\n".format(indir))
       print "Number of high usage codes: ", ntop
       if uscale > 1:
          print "Cores per node: ", uscale
       if group is not None:
          print "Restricting usage to group: " + group

    # Check the directory exists
    if not os.path.isdir(indir):
       print "**ERROR - Log directory is not valid: " + indir
       printHelp(rootDir)
       exit(0)

    # Read the logfile directory
    files = get_filelist(indir, 'apstat')

    # Initialise the arrays for storing the time sequence
    datearray = []
    meanjobsize =[]
    tot_thours = []
    tot_tjobs = []
    other_meanjob = []
    other_thours = []
    other_tjobs = []
    code_meanjob = {}
    code_thours = {}
    code_tjobs = {}
    for code in codes:
       name = code.name
       code_meanjob[name] = []
       code_thours[name] = []
       code_tjobs[name] = []

    daycounter = 0
    nfilejobs = 0
    nfilenodeh = 0
    nfilenodehsq = 0
    code_filenh = {}
    code_filenhsq = {}
    code_filejobs = {}
    for code in codes:
       name = code.name
       code_filenh[name] = 0 
       code_filejobs[name] = 0 
       code_filenhsq[name] = 0
    other_filenh = 0
    other_filenhsq = 0
    other_filejobs = 0
   
    startdate = None
    enddate = None

    # Open CSV files for time series data
    if csv:
       totcsv = open("tot_ts.csv", "w")
       totcsv.write("{0},{1},{2},{3}\n".format("Date", "Mean Job Size", "Total Usage", "Number of Jobs"))
    for file in files:
     filename = os.path.basename(file)
     filedate = filename.split('.')[0]
     fdate = datetime.strptime(filedate, "%Y-%m-%d")
     if fdate >= startfile and fdate <= endfile:
         if startdate is None: startdate = fdate
         enddate = fdate
         # This is where we will read and process data
         logFile = open(file, "r")

         # Zero counters for timeline
         if daycounter == 0:
            nfilejobs = 0
            nfilenodeh = 0
            nfilenodehsq = 0
            for code in codes:
               name = code.name
               code_filenh[name] = 0 
               code_filenhsq[name] = 0
               code_filejobs[name] = 0 
            other_filenh = 0
            other_filenhsq = 0
            other_filejobs = 0

         # Initialise variables
         intimerange = True
         # Loop over lines in the file reading them
         for line in logFile:

            if re.match("__START", line) is not None:
               timeline = line.rstrip()
               tokens = timeline.split()
            # End of if start block match

            # If we are in the time range then accumulate data
            if intimerange:
               
               # Skip the end indicator
               if re.match("__END", line) is not None:
                  continue


               # Extract the useful information from the log
               line = line.rstrip()
               tokens = line.split()

               if len(tokens) < 8:
                   continue
               jobid = tokens[0]
               user = tokens[2]
               nodes = tokens[4]
               exename = tokens[7]

               # We may need to test if we are counting this job
               addjob = True
               if byuser:
                  # Space added to try and counter substring username matches
                  if '"' + user + '"' in userlist:
                     addjob = True
                  else:
                     addjob = False

               # This is needed in case we are filtering by user
               if addjob:
                  # Add this number of hours to total
                  totnh += int(nodes)
                  nfilenodeh += int(nodes)
                  nfilenodehsq += int(nodes) * int(nodes)

                  # Check we have not already seen this job
                  countjob = True
                  if jobid in matchedjobs:
                     countjob = False
                  else:
                     totjobs += 1
                     nfilejobs += 1
                     matchedjobs[jobid] = True
   
                  # Check if we have already seen this user
                  if user not in users:
                     users = "{0} {1}".format(users, user)
                     totusers += 1
   
                  # Store this job by size
                  if nodes in tot_nh_bysize:
                     tot_nh_bysize[nodes] += int(nodes)
                     if countjob:
                        tot_jobs_bysize[nodes] += 1
                  else:
                     tot_nh_bysize[nodes] = int(nodes)
                     if countjob:
                        tot_jobs_bysize[nodes] = 1
                  # Loop over codes, storing this job
                  matched = False
                  for code in codes:
                     name = code.name
                     if re.search(code_regexp[name], exename):
                        matched = True
                        code_nh[name] += int(nodes)
                        if countjob:
                           code_jobs[name] += 1
                           code_filejobs[name] += 1
                        if user not in code_users[name]:
                           code_users[name] = "{0} {1}".format(code_users[name], user)
                           code_num_users[name] += 1
                        code_filenh[name] += int(nodes)
                        code_filenhsq[name] += int(nodes)*int(nodes)
                        if (name, nodes) in code_nh_bysize:
                           code_nh_bysize[(name, nodes)] += int(nodes)
                           if countjob:
                              code_jobs_bysize[(name, nodes)] += 1
                        else:
                           code_nh_bysize[(name, nodes)] = int(nodes)
                           if countjob:
                              code_jobs_bysize[(name, nodes)] = 1
                  # End of loop over codes
   
                  # Store if not matched to defined code
                  if not matched:
                     other_nh += int(nodes)
                     if exename in other_codes:
                        other_codes[exename] += int(nodes)
                     else:
                        other_codes[exename] = int(nodes)
                     if countjob:
                        other_jobs += 1
                        other_filejobs += 1
                     if user not in other_users:
                        other_users = "{0} {1}".format(other_users, user)
                        other_num_users += 1
                     other_filenh += int(nodes)
                     other_filenhsq += int(nodes)*int(nodes)
                     if nodes in other_nh_bysize:
                        other_nh_bysize[nodes] += int(nodes)
                        if countjob:
                           other_jobs_bysize[nodes] += 1
                     else:
                        other_nh_bysize[nodes] = int(nodes)
                        if countjob:
                           other_jobs_bysize[nodes] = 1
                # End of if countjob
 
            # End of if intimerange

         # End of loop over logfile lines
         
         # Close the logfile
         logFile.close()

         daycounter += 1
         if daycounter == day_interval:
            datearray.append(fdate)
            if nfilenodeh > 0:
               mjobsize = uscale * float(nfilenodehsq)/float(nfilenodeh)
               meanjobsize.append(mjobsize)
               tot_thours.append(uscale * nfilenodeh)
               tot_tjobs.append(nfilejobs)
               if csv:
                  totcsv.write("{0},{1:.4f},{2:d},{3:d}\n".format(filedate, mjobsize, uscale*nfilenodeh, nfilejobs))
            else:
               meanjobsize.append(0)
               tot_thours.append(0)
               tot_tjobs.append(0)
               if csv:
                  totcsv.write("{0},{1:.4f},{2d},{3:d}\n".format(filedate, 0, 0, 0))
            for code in codes:
               name = code.name
               if code_filenh[name] > 0:
                  code_meanjob[name].append(uscale * float(code_filenhsq[name])/float(code_filenh[name]))
                  code_thours[name].append(uscale * code_filenh[name])
                  code_tjobs[name].append(code_filejobs[name])
               else:
                  code_meanjob[name].append(0)
                  code_thours[name].append(0)
                  code_tjobs[name].append(0)
            if other_filenh > 0:
               other_meanjob.append(uscale * float(other_filenhsq)/float(other_filenh))
               other_thours.append(uscale * other_filenh)
               other_tjobs.append(other_filejobs)
            else:
               other_meanjob.append(0)
               other_thours.append(0)
               other_tjobs.append(0)
            daycounter = 0

    # End of loop over logfiles
    if csv:
       totcsv.close()

    if text:
       sys.stdout.write("\n Started at: " + startdate.strftime("%Y-%m-%d") + "\n")
       sys.stdout.write("Finished at: " + enddate.strftime("%Y-%m-%d") + "\n")

    # Get list of the top 10 codes by node hours
    top_codes = [k for k in sorted(code_nh, key=code_nh.get, reverse=True)[:ntop]]

    colmap = None
    cNorm = None
    scalarMap = None
    pie_colmap = None
    pie_cNorm = None
    pie_scalarMap = None
    if png: 
       import matplotlib
       matplotlib.rcParams['font.size'] = 9
       matplotlib.use("Agg")
       from matplotlib import pyplot as plt
       import numpy as np
       from matplotlib import dates
       import matplotlib.colors as colors
       import matplotlib.cm as cmx
       import matplotlib.patches as mpatches

       # Set up a nice colormap to vary the line colours
       colmap = cm = plt.get_cmap('spectral') 
       cNorm  = colors.Normalize(vmin=0, vmax=ntop+1)
       scalarMap = cmx.ScalarMappable(norm=cNorm, cmap=colmap)

       # Set up a colormap for pie charts
       pie_colmap = cm = plt.get_cmap('Pastel2') 
       pie_cNorm  = colors.Normalize(vmin=0, vmax=ntop+3)
       pie_scalarMap = cmx.ScalarMappable(norm=pie_cNorm, cmap=pie_colmap)

       # Plot a time profile of the overall mean job size
       fig = plt.figure(1)
       ax = plt.subplot(1, 1, 1)
       ax.cla()
       idx = 0
       recs = []
       labels = []
       cursum = []
       newsum = []
       for name in top_codes:
          if code_nh[name] > 0:
             colorVal = scalarMap.to_rgba(idx)
             recs.append(mpatches.Rectangle((0, 0), 1, 1, fc=colorVal))
             labels.append(name)
             if idx == 0:
                ax.fill_between(datearray, 0, code_meanjob[name], facecolor=colorVal, color=colorVal)
                cursum = list(code_meanjob[name])
                newsum = list(cursum)
             else:
                for i, bi in enumerate(code_meanjob[name]): newsum[i] = cursum[i] + bi
                ax.fill_between(datearray, cursum, newsum, facecolor=colorVal, color=colorVal)
                cursum = list(newsum)
             idx += 1
       if other_nh > 0:
          for i, bi in enumerate(other_meanjob): newsum[i] = cursum[i] + bi
          ax.fill_between(datearray, cursum, newsum, facecolor='0.5', color='0.5')
          recs.append(mpatches.Rectangle((0, 0), 1, 1, fc='0.5'))
          labels.append('Unknown')
          cursum = list(newsum)
       for i, bi in enumerate(meanjobsize): newsum[i] = cursum[i] + bi
       ax.fill_between(datearray, cursum, newsum, facecolor='0.75', color='0.75')
       recs.append(mpatches.Rectangle((0, 0), 1, 1, fc='0.75'))
       labels.append('All')
       box = ax.get_position()
       ax.xaxis.set_major_formatter(dates.DateFormatter("%Y-%m-%d"))
       fig.autofmt_xdate()
       ax.set_xlabel('Date')
       if units == "nodes":
          ax.set_ylabel('Mean Job Size / Nodes')
          ax.set_title('Mean job size over time')
       elif units == "cores":
          ax.set_ylabel('Mean Job Size / Cores')
          ax.set_title('Mean job size over time')
       ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])
       plt.legend(recs, labels, bbox_to_anchor=(1.4, 1.0))
       fig.savefig("{0}/meanjob_timeline.png".format(outDir))

       # Plot a time profile of the overall node hours
       fig = plt.figure(1)
       ax = plt.subplot(1, 1, 1)
       ax.cla()
       idx = 0
       recs = []
       labels = []
       cursum = []
       newsum = []
       for name in top_codes:
          if code_nh[name] > 0:
             colorVal = scalarMap.to_rgba(idx)
             recs.append(mpatches.Rectangle((0, 0), 1, 1, fc=colorVal))
             labels.append(name)
             if idx == 0:
                ax.fill_between(datearray, 0, code_thours[name], facecolor=colorVal, color=colorVal)
                cursum = list(code_thours[name])
                newsum = list(cursum)
             else:
                for i, bi in enumerate(code_thours[name]): newsum[i] = cursum[i] + bi
                ax.fill_between(datearray, cursum, newsum, facecolor=colorVal, color=colorVal)
                cursum = list(newsum)
             idx += 1
       if other_nh > 0:
          for i, bi in enumerate(other_thours): newsum[i] = cursum[i] + bi
          ax.fill_between(datearray, cursum, newsum, facecolor='0.5', color='0.5')
          recs.append(mpatches.Rectangle((0, 0), 1, 1, fc='0.5'))
          labels.append('Unknown')
          cursum = list(newsum)
       ax.plot(datearray, tot_thours, 'r-')
#     ax.fill_between(datearray, cursum, tot_thours, facecolor='0.75', color='0.75')
       recs.append(mpatches.Rectangle((0, 0), 1, 1, fc='r'))
       labels.append('All')
       ax.xaxis.set_major_formatter(dates.DateFormatter("%Y-%m-%d"))
       ax.set_xlabel('Date')
       ax.set_ylabel('Usage / ' + punit)
       ax.set_title('Usage over time')
       fig.autofmt_xdate()
       ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])
       plt.legend(recs, labels, bbox_to_anchor=(1.4, 1.0))
       fig.savefig("{0}/hours_timeline.png".format(outDir))
    # End: if png

    # Bin the job sizes into the specified size bins and compute the mean job sizes
    # The tot arrays should contain keys with all the node counts encountered

    # Initialise median calculators
    tot_sumh = 0
    tot_medh = 0.0
    other_sumh = 0
    other_medh = 0.0
    code_sumh = {}
    code_medh = {}
    for code in codes:
       name = code.name
       code_sumh[name] = 0
       code_medh[name] = 0.0
    # Initialise all the "bin" arrays
    btot_nh_bysize = {}
    btot_jobs_bysize = {}
    bother_nh_bysize = {}
    bother_jobs_bysize = {}
    bcode_nh_bysize = {}
    bcode_jobs_bysize = {}
    tot_sum_jobs_size = 0
    other_sum_jobs_size = 0
    code_sum_jobs_size = {}
    for size in sizes:
       btot_nh_bysize[size] = 0
       btot_jobs_bysize[size] = 0
       bother_nh_bysize[size] = 0
       bother_jobs_bysize[size] = 0
    for code in codes:
       name = code.name
       code_sum_jobs_size[name] = 0
       # Use tuple dict keys as they are less of a pain than 2D dicts
       for size in sizes:
          bcode_nh_bysize[(name, size)] = 0
          bcode_jobs_bysize[(name, size)] = 0
    # Loop over all sizes encountered binning them up
    for nodes in sorted(tot_nh_bysize, key=int):
       tot_sum_jobs_size += int(nodes) * int(tot_nh_bysize[nodes])
       tot_sumh += int(tot_nh_bysize[nodes])
       if tot_sumh < (totnh/2.0):
          tot_medh = nodes
       for size in sizes:
          if int(nodes) <= int(size):
             btot_nh_bysize[size] += int(tot_nh_bysize[nodes])
             btot_jobs_bysize[size] += int(tot_jobs_bysize[nodes])
             if nodes in other_nh_bysize:
                other_sumh += int(tot_nh_bysize[nodes])
                if other_sumh < other_nh / 2.0:
                   other_medh = nodes
                other_sum_jobs_size += int(nodes) * int(other_nh_bysize[nodes])
                bother_nh_bysize[size] += int(other_nh_bysize[nodes])
                bother_jobs_bysize[size] += int(other_jobs_bysize[nodes])
             for code in codes:
                name = code.name
                if (name, nodes) in code_nh_bysize:
                   code_sumh[name] += code_nh_bysize[(name, nodes)]
                   if code_sumh[name] < code_nh[name]/2.0:
                      code_medh[name] = nodes
                   code_sum_jobs_size[name] += int(nodes) * int(code_nh_bysize[(name, nodes)])
                   bcode_nh_bysize[(name, size)] += int(code_nh_bysize[(name, nodes)])
                   bcode_jobs_bysize[(name, size)] += int(code_jobs_bysize[(name, nodes)])
             break

    # Node hours by job size
    if text:
       if units == "nodes":
          sys.stdout.write("\n\nNode hours by job size (in nodes)\n")
       elif units == "cores":
          sys.stdout.write("\n\nCPUh by job size (in cores)\n")
       head = "{0:>20s}".format("Code")
       lines = "{0:>20s}".format("====")
       for size in size_labels:
          head = head + "{0:>20s}".format(size)
          lines = lines + "{0:>20s}".format("=====")
       limits = "{0:>20s}".format("")
       for size in sizes:
          t = "<{0:d} {1}".format(uscale * size, units)
          limits = limits + "{0:>20s}".format(t)
       sys.stdout.write(head + "\n" + limits + "\n" + lines + "\n")
       for name in sorted(code_nh, key=code_nh.get, reverse=True):
          sys.stdout.write("{0:>20s}".format(name))
          for size in sizes:
             sys.stdout.write("{0:20d}".format(uscale * bcode_nh_bysize[(name, size)]))
          sys.stdout.write("\n")
       sys.stdout.write("{0:>20s}".format("Unknown"))
       for size in sizes:
          sys.stdout.write("{0:>20d}".format(uscale * bother_nh_bysize[size]))
       sys.stdout.write("\n")
       lines = "{0:>20s}".format("----")
       for size in sizes:
          lines = lines + "{0:>20s}".format("-----")
       sys.stdout.write(lines + "\n")
       sys.stdout.write("{0:>20s}".format("Total"))
       for size in sizes:
          sys.stdout.write("{0:>20d}".format(uscale * btot_nh_bysize[size]))
       sys.stdout.write("\n")
       lines = "{0:>20s}".format("====")
       for size in sizes:
          lines = lines + "{0:>20s}".format("====")
       sys.stdout.write(lines + "\n")

    # Plot bar chart of usage against size
    if png:
       bar_labels = []
       for i, bi in enumerate(size_labels):
          bar_labels.append("{0}\n<{1:d} {2}".format(bi, uscale * sizes[i], units))
       width = 0.7
       fig = plt.figure(1)
       fig.clf()
       ax = plt.subplot(1, 1, 1)
       nrow = len(btot_nh_bysize)
       data = []
       for size in sizes:
          data.append(uscale * btot_nh_bysize[size])
       pos = np.arange(nrow) - (width/2.0)
       ax.bar(pos, data, width, color='0.75', alpha=0.75)
       ax.set_xticks(np.arange(nrow))
       ax.set_xticklabels(bar_labels)
       ax.set_title('Usage by Job Size')
       ax.set_ylabel('Usage / ' + punit)
       fig.savefig("{0}/usagebysize_Total.png".format(outDir))

       # Bar charts for each code
       idx = 0
       for name in top_codes:
          fig = plt.figure(1)
          ax = plt.subplot(1, 1, 1)
          ax.cla()
          data = []
          for size in sizes:
             data.append(uscale * bcode_nh_bysize[(name,size)])
          colorVal = scalarMap.to_rgba(idx)
          ax.bar(pos, data, width, color=colorVal, edgecolor=colorVal, alpha=0.75)
          idx += 1
          ax.set_xticks(np.arange(nrow))
          ax.set_xticklabels(bar_labels)
          ax.set_xlim([-1,nrow])
          ax.set_title('Usage by Job Size for ' + name)
          ax.set_ylabel('Usage / ' + punit)
          fig.savefig("{0}/usagebysize_{1}.png".format(outDir, name))
    # End: if png

    # Jobs by job size
    if text:
       sys.stdout.write("\n\nNumber of jobs by job size\n")
       head = "{0:>20s}".format("Code")
       lines = "{0:>20s}".format("====")
       for size in size_labels:
          head = head + "{0:>20s}".format(size)
          lines = lines + "{0:>20s}".format("=====")
       limits = "{0:>20s}".format("")
       for size in sizes:
          t = "<{0:d} {1}".format(uscale * size, units)
          limits = limits + "{0:>20s}".format(t)
       sys.stdout.write(head + "\n" + limits + "\n" + lines + "\n")
       for name in sorted(code_nh, key=code_nh.get, reverse=True):
          sys.stdout.write("{0:>20s}".format(name))
          for size in sizes:
             sys.stdout.write("{0:20d}".format(bcode_jobs_bysize[(name, size)]))
          sys.stdout.write("\n")
       sys.stdout.write("{0:>20s}".format("Unknown"))
       for size in sizes:
          sys.stdout.write("{0:>20d}".format(bother_jobs_bysize[size]))
       sys.stdout.write("\n")
       lines = "{0:>20s}".format("----")
       for size in sizes:
          lines = lines + "{0:>20s}".format("-----")
       sys.stdout.write(lines + "\n")
       sys.stdout.write("{0:>20s}".format("Total"))
       for size in sizes:
          sys.stdout.write("{0:>20d}".format(btot_jobs_bysize[size]))
       sys.stdout.write("\n")
       lines = "{0:>20s}".format("====")
       for size in sizes:
          lines = lines + "{0:>20s}".format("====")
       sys.stdout.write(lines + "\n")

    # Loop over codes printing usage
    if png:
       fig = plt.figure(1)
       fig.clf()
       ax = plt.subplot(1, 1, 1)
       pie_labels = []
       pie_vals = []
       pie_cols = []
       pie_explode = []
       sum_nh = 0
       idx = 0
       for name in top_codes:
          pie_cols.append(pie_scalarMap.to_rgba(idx))
          pie_labels.append(name)
          pie_vals.append(float(code_nh[name]))
          pie_explode.append(0.05)
          sum_nh += code_nh[name]
          idx += 1
       pie_cols.append(pie_scalarMap.to_rgba(idx))
       pie_labels.append("Other Known Codes")
       pie_vals.append(totnh - sum_nh - other_nh)
       pie_explode.append(0.05)
       idx += 1
       pie_cols.append(pie_scalarMap.to_rgba(idx))
       pie_labels.append("Unidentified Codes")
       pie_vals.append(other_nh)
       pie_explode.append(0.05)
       ax.pie(pie_vals, explode=pie_explode, labels=pie_labels, colors=pie_cols, autopct='%1.1f%%')
       ax.axis('equal')
       fig.savefig("{0}/usage_pie.png".format(outDir))
    if text:
       if units == 'nodes':
          sys.stdout.write("\n\nTotal code usage (ordered by node hours)\n")
       elif units == 'cores':
          sys.stdout.write("\n\nTotal code usage (ordered by CPUh)\n")
         
       sys.stdout.write("{0:>20s}{1:>15s}{2:>10s}{3:>10s}{4:>10s}{5:>10s}{6:>10s}{7:>10s}\n".format("Code", punit, "% Time", "Jobs", "% Jobs", "Users", "Mean", "Median"))
       sys.stdout.write("{0:>20s}{1:>15s}{2:>10s}{3:>10s}{4:>10s}{5:>10s}{6:>10s}{7:>10s}\n".format("====", "==========", "======", "====", "======", "=====", "====", "======"))
       for name in sorted(code_nh, key=code_nh.get, reverse=True):
          nhfrac = 100 * float(code_nh[name])/float(totnh)
          jobfrac = 100 * float(code_jobs[name])/float(totjobs)
          meanjob = 0.0
          if code_nh[name] > 0:
             meanjob = float(code_sum_jobs_size[name]) / float(code_nh[name])
          sys.stdout.write("{0:>20s}{1:15d}{2:10.2f}{3:10d}{4:10.2f}{5:10d}{6:10.2f}{7:10d}\n".format(name, uscale * code_nh[name], nhfrac, code_jobs[name], jobfrac, code_num_users[name],uscale * meanjob, uscale * int(code_medh[name])))
       nhfrac = 100 * float(other_nh)/float(totnh)
       jobfrac = 100 * float(other_jobs)/float(totjobs)
       meanjob = 0.0
       if other_nh > 0:
          meanjob = float(other_sum_jobs_size)/float(other_nh)
       sys.stdout.write("{0:>20s}{1:15d}{2:10.2f}{3:10d}{4:10.2f}{5:10d}{6:10.2f}{7:10d}\n".format("Unknown", uscale * other_nh, nhfrac, other_jobs, jobfrac, other_num_users, uscale * meanjob, uscale * int(other_medh)))
       sys.stdout.write("{0:>20s}{1:>15s}{2:>10s}{3:>10s}{4:>10s}{5:>10s}{6:>10s}{7:>10s}\n".format("----", "----------", "------", "----", "------", "-----", "----", "------"))
       nhfrac = 100 * float(totnh)/float(totnh)
       jobfrac = 100 * float(totjobs)/float(totjobs)
       meanjob = 0.0
       if totnh > 0:
          meanjob = float(tot_sum_jobs_size)/float(totnh)
       sys.stdout.write("{0:>20s}{1:15d}{2:10.2f}{3:10d}{4:10.2f}{5:10d}{6:10.2f}{7:10d}\n".format("Total", uscale * totnh, nhfrac, totjobs, jobfrac, totusers, uscale * meanjob, uscale * int(tot_medh)))
       sys.stdout.write("{0:>20s}{1:>15s}{2:>10s}{3:>10s}{4:>10s}{5:>10s}{6:>10s}{7:>10s}\n".format("====", "==========", "======", "====", "======", "=====", "====", "======"))

    # Write HTML table if required
    if html:
       htmlfile = open("top.html", "w")
       if units == 'nodes':
          htmlfile.write("<p>Code usage for top {0:d} codes (ordered by node hours)</p>\n".format(ntop))
       elif units == 'cores':
          htmlfile.write("<p>Code usage for top {0:d} codes (ordered by CPUh)</p>\n".format(ntop))
       htmlfile.write("""
<table class="cse-table">
   <tr>
      <th>Code Name</th>
      <th>{0}</th>
      <th>% {0}</th>
      <th>Jobs</th>
      <th>% Jobs</th>
      <th>Users</th>
      <th>Mean Job Size</th>
      <th>Median Job Size</th>
   </tr>
""".format(punit))
       for name in top_codes:
          nhfrac = 100 * float(code_nh[name])/float(totnh)
          jobfrac = 100 * float(code_jobs[name])/float(totjobs)
          meanjob = 0.0
          if code_nh[name] > 0:
             meanjob = float(code_sum_jobs_size[name]) / float(code_nh[name])
          htmlfile.write("""
   <tr>      
      <td>{0}</td>
      <td>{1:d}</td>
      <td>{2:.2f}</td>
      <td>{3:d}</td>
      <td>{4:.2f}</td>
      <td>{5:d}</td>
      <td>{6:.2f}</td>
      <td>{7:d}</td>
   </tr>      
""".format(name, uscale * code_nh[name], nhfrac, code_jobs[name], jobfrac, code_num_users[name],uscale * meanjob, uscale * int(code_medh[name])))
       htmlfile.write("</table>\n")
       htmlfile.close()

    if csv:
       csvfile = open("code_summary.csv", "w")
       csvfile.write('"{0}","{1}","{2}","{3}","{4}","{5}","{6}","{7}"\n'.format("Code", punit, "% Time", "Jobs", "% Jobs", "Users", "Mean", "Median"))
       for name in sorted(code_nh, key=code_nh.get, reverse=True):
          nhfrac = 100 * float(code_nh[name])/float(totnh)
          jobfrac = 100 * float(code_jobs[name])/float(totjobs)
          meanjob = 0.0
          if code_nh[name] > 0:
             meanjob = float(code_sum_jobs_size[name]) / float(code_nh[name])
          csvfile.write('"{0}",{1:d},{2:.2f},{3:d},{4:.2f},{5:d},{6:.2f},{7:d}\n'.format(name, uscale * code_nh[name], nhfrac, code_jobs[name], jobfrac, code_num_users[name],uscale * meanjob, uscale * int(code_medh[name])))
       nhfrac = 100 * float(other_nh)/float(totnh)
       jobfrac = 100 * float(other_jobs)/float(totjobs)
       meanjob = 0.0
       if other_nh > 0:
          meanjob = float(other_sum_jobs_size)/float(other_nh)
          csvfile.write('"{0}",{1:d},{2:.2f},{3:d},{4:.2f},{5:d},{6:.2f},{7:d}\n'.format("Unknown", uscale * other_nh, nhfrac, other_jobs, jobfrac, other_num_users,uscale * meanjob, uscale * int(other_medh)))
       nhfrac = 100 * float(totnh)/float(totnh)
       jobfrac = 100 * float(totjobs)/float(totjobs)
       meanjob = 0.0
       if totnh > 0:
          meanjob = float(tot_sum_jobs_size)/float(totnh)
          csvfile.write('"{0}",{1:d},{2:.2f},{3:d},{4:.2f},{5:d},{6:.2f},{7:d}\n'.format("Total", uscale * totnh, nhfrac, totjobs, jobfrac, totusers,uscale * meanjob, uscale * int(tot_medh)))
       csvfile.close()


    # Aggregate results on code metadata
    lang_nh = {}
    lang_jobs = {}
    for code in codes:
       name = code.name
       lang = code.pri_lang
       if lang in lang_nh:
          lang_nh[lang] += code_nh[name]
          lang_jobs[lang] += code_jobs[name]
       else:
          lang_nh[lang] = code_nh[name]
          lang_jobs[lang] = code_jobs[name]
    # Loop over codes printing usage
    if text:
       if units == 'nodes':
          sys.stdout.write("\n\nTotal language usage (ordered by node hours)\n")
       elif units == 'cores':
          sys.stdout.write("\n\nTotal language usage (ordered by CPUh)\n")
       sys.stdout.write("{0:>20s}{1:>15s}{2:>10s}{3:>10s}{4:>10s}\n".format("Code", punit, "% Time", "Jobs", "% Jobs"))
       sys.stdout.write("{0:>20s}{1:>15s}{2:>10s}{3:>10s}{4:>10s}\n".format("====", "==========", "======", "====", "======"))
       for lang in sorted(lang_nh, key=lang_nh.get, reverse=True):
          nhfrac = 100 * float(lang_nh[lang])/float(totnh)
          jobfrac = 100 * float(lang_jobs[lang])/float(totjobs)
          sys.stdout.write("{0:>20s}{1:15d}{2:10.2f}{3:10d}{4:10.2f}\n".format(lang, uscale * lang_nh[lang], nhfrac, lang_jobs[lang], jobfrac))
       nhfrac = 100 * float(other_nh)/float(totnh)
       jobfrac = 100 * float(other_jobs)/float(totjobs)
       sys.stdout.write("{0:>20s}{1:15d}{2:10.2f}{3:10d}{4:10.2f}\n".format("Unknown", uscale * other_nh, nhfrac, other_jobs, jobfrac))
       sys.stdout.write("{0:>20s}{1:>15s}{2:>10s}{3:>10s}{4:>10s}\n".format("----", "----------", "------", "----", "------"))
       nhfrac = 100 * float(totnh)/float(totnh)
       jobfrac = 100 * float(totjobs)/float(totjobs)
       sys.stdout.write("{0:>20s}{1:15d}{2:10.2f}{3:10d}{4:10.2f}\n".format("Total", uscale * totnh, nhfrac, totjobs, jobfrac))
       sys.stdout.write("{0:>20s}{1:>15s}{2:>10s}{3:>10s}{4:>10s}\n".format("====", "==========", "======", "====", "======"))

    # Aggregate results on type metadata
    meta_nh = {}
    meta_jobs = {}
    for code in codes:
       name = code.name
       meta = code.type
       if meta in meta_nh:
          meta_nh[meta] += code_nh[name]
          meta_jobs[meta] += code_jobs[name]
       else:
          meta_nh[meta] = code_nh[name]
          meta_jobs[meta] = code_jobs[name]
    # Loop over codes printing usage
    if text:
       if units == 'nodes':
          sys.stdout.write("\n\nTotal type usage (ordered by node hours)\n")
       elif units == 'cores':
          sys.stdout.write("\n\nTotal type usage (ordered by CPUh)\n")
       sys.stdout.write("{0:>30s}{1:>15s}{2:>10s}{3:>10s}{4:>10s}\n".format("Code", punit, "% Time", "Jobs", "% Jobs"))
       sys.stdout.write("{0:>30s}{1:>15s}{2:>10s}{3:>10s}{4:>10s}\n".format("====", "==========", "======", "====", "======"))
       for meta in sorted(meta_nh, key=meta_nh.get, reverse=True):
          nhfrac = 100 * float(meta_nh[meta])/float(totnh)
          jobfrac = 100 * float(meta_jobs[meta])/float(totjobs)
          sys.stdout.write("{0:>30s}{1:15d}{2:10.2f}{3:10d}{4:10.2f}\n".format(meta, uscale * meta_nh[meta], nhfrac, meta_jobs[meta], jobfrac))
       nhfrac = 100 * float(other_nh)/float(totnh)
       jobfrac = 100 * float(other_jobs)/float(totjobs)
       sys.stdout.write("{0:>30s}{1:15d}{2:10.2f}{3:10d}{4:10.2f}\n".format("Unknown", uscale * other_nh, nhfrac, other_jobs, jobfrac))
       sys.stdout.write("{0:>30s}{1:>15s}{2:>10s}{3:>10s}{4:>10s}\n".format("----", "----------", "------", "----", "------"))
       nhfrac = 100 * float(totnh)/float(totnh)
       jobfrac = 100 * float(totjobs)/float(totjobs)
       sys.stdout.write("{0:>30s}{1:15d}{2:10.2f}{3:10d}{4:10.2f}\n".format("Total", uscale * totnh, nhfrac, totjobs, jobfrac))
       sys.stdout.write("{0:>30s}{1:>15s}{2:>10s}{3:>10s}{4:>10s}\n".format("====", "==========", "======", "====", "======"))

    # Aggregate results on type metadata
    meta_nh = {}
    meta_jobs = {}
    for code in codes:
       name = code.name
       meta = code.area
       if meta in meta_nh:
          meta_nh[meta] += code_nh[name]
          meta_jobs[meta] += code_jobs[name]
       else:
          meta_nh[meta] = code_nh[name]
          meta_jobs[meta] = code_jobs[name]
    if png:
       fig = plt.figure(1)
       fig.clf()
       ax = plt.subplot(1, 1, 1)
       pie_labels = []
       pie_vals = []
       pie_cols = []
       pie_explode = []
       sum_nh = 0
       idx = 0
       for meta in sorted(meta_nh, key=meta_nh.get, reverse=True):
          pie_cols.append(pie_scalarMap.to_rgba(idx))
          pie_labels.append(meta)
          pie_vals.append(float(meta_nh[meta]))
          pie_explode.append(0.05)
          idx += 1
       ax.pie(pie_vals, explode=pie_explode, labels=pie_labels, colors=pie_cols, autopct='%1.1f%%')
       ax.axis('equal')
       fig.savefig("{0}/resarea_pie.png".format(outDir))
    # Loop over codes printing usage
    if text:
       if units == 'nodes':
          sys.stdout.write("\n\nTotal research area usage (ordered by node hours)\n")
       elif units == 'cores':
          sys.stdout.write("\n\nTotal research area usage (ordered by CPUh)\n")
       sys.stdout.write("{0:>30s}{1:>15s}{2:>10s}{3:>10s}{4:>10s}\n".format("Code", punit, "% Time", "Jobs", "% Jobs"))
       sys.stdout.write("{0:>30s}{1:>15s}{2:>10s}{3:>10s}{4:>10s}\n".format("====", "==========", "======", "====", "======"))
       for meta in sorted(meta_nh, key=meta_nh.get, reverse=True):
          nhfrac = 100 * float(meta_nh[meta])/float(totnh)
          jobfrac = 100 * float(meta_jobs[meta])/float(totjobs)
          sys.stdout.write("{0:>30s}{1:15d}{2:10.2f}{3:10d}{4:10.2f}\n".format(meta, uscale * meta_nh[meta], nhfrac, meta_jobs[meta], jobfrac))
       nhfrac = 100 * float(other_nh)/float(totnh)
       jobfrac = 100 * float(other_jobs)/float(totjobs)
       sys.stdout.write("{0:>30s}{1:15d}{2:10.2f}{3:10d}{4:10.2f}\n".format("Unknown", uscale * other_nh, nhfrac, other_jobs, jobfrac))
       sys.stdout.write("{0:>30s}{1:>15s}{2:>10s}{3:>10s}{4:>10s}\n".format("----", "----------", "------", "----", "------"))
       nhfrac = 100 * float(totnh)/float(totnh)
       jobfrac = 100 * float(totjobs)/float(totjobs)
       sys.stdout.write("{0:>30s}{1:15d}{2:10.2f}{3:10d}{4:10.2f}\n".format("Total", uscale * totnh, nhfrac, totjobs, jobfrac))
       sys.stdout.write("{0:>30s}{1:>15s}{2:>10s}{3:>10s}{4:>10s}\n".format("====", "==========", "======", "====", "======"))

    # Print out any other executables with usage over defined threshold
    percent = thresh * float(totnh)
    if text:
       if units == 'nodes':
          sys.stdout.write("\n\nUndefined executables with usage greater than {0:5.2f}% of total node hours:\n".format(100*thresh))
       elif units == 'cores':
          sys.stdout.write("\n\nUndefined executables with usage greater than {0:5.2f}% of total CPUh:\n".format(100*thresh))
       sys.stdout.write("{0:>30s}{1:>15s}{2:>10s}\n".format("Code", punit, "% Time"))
       sys.stdout.write("{0:>30s}{1:>15s}{2:>10s}\n".format("====", "==========", "======"))
       other_tot = 0
       for name in sorted(other_codes, key=other_codes.get, reverse=True):
          if other_codes[name] > percent:
             nhfrac = 100 * float(other_codes[name])/float(totnh)
             other_tot += int(other_codes[name])
             sys.stdout.write("{0:>30s}{1:15d}{2:10.2f}\n".format(name, uscale * other_codes[name], nhfrac))
       sys.stdout.write("{0:>30s}{1:>15s}{2:>10s}\n".format("----", "----------", "------"))
       nhfrac = 100 * float(other_tot)/float(totnh)
       sys.stdout.write("{0:>30s}{1:15d}{2:10.2f}\n".format("Total", uscale * other_tot, nhfrac))
       sys.stdout.write("{0:>30s}{1:>15s}{2:>10s}\n".format("====", "==========", "======"))


    # Finish nicely
    exit(0)

def printHelp(rootDir):
    """Print help for the tool.
           
           Arguments:
              str rootDir - The root install directory of the tool.
        """

    print """Get code usage statistics from specified log file.

USAGE

code_usage [options]

OPTIONS

-l,--logdir <log directory> Directory containing the logfiles to analyse. The
                            default is:
                            /home/z01/z01/archstat/archer-monitor/logs/applications

-c,--cores  <number>        Report data in CPUh instead of node hours (Nh). The
                            number specifies the number of cores per node for the
                            conversion.

-s,--start  yyyy-mm-dd      Start date for analysis (default is start of data in logfiles) 
-e,--end    yyyy-mm-dd      End date for analysis (default is end of data in logfiles) 

-g,--group  <group>         Restrict analysis to specified group (e.g. e05)

-b,--bins   <filename>      Specify the histogram bins for binning the data as 
                            a function of job size. The ASCII file specified should
                            contain two lines: the fist line is a space separated 
                            list of the upper limits of the bins (in nodes), the
                            second line is a space separted list of the labels for
                            the bins. The two lines must contain the same number
                            of items. For example:

                            512 4920
                            Small Large

                            If this option is not specified then the default binning
                            will be used. This is equivalent to a file with:
  
                            4, 64, 256, 512, 4920
                            Small Medium Large V.Large Huge

-h,--help                   Show this help.
"""

if __name__ == "__main__":
    main(sys.argv[1:])
